## 2023-08-12

Another day..
Another SOTA LLM..

New SOTA: OpenOrca-Platypus2-13B
An ensemble of two powerful models: Orca &amp; Platypus2

Link: https://t.co/N2QYU0iNLV

Orca TL;DR: "Add detailed step-by-step explanations to each of the responses."
- https://t.co/BTJOzJkTCS

Platypus TL;DR: "Train‚Ä¶ https://t.co/OLaqhr2yGU

## 2023-08-12

The most powerful open source instructions dataset:

Flan.
378 Million samples. (~300GB) [1]

- Link: https://t.co/2OSCy2ofWg

Why should you care? ü§î

- Flan is an incredibly powerful dataset [2] and some famous models trained on it (FlanT5, UL2..) hold the top positions on‚Ä¶ https://t.co/XYRlxuVTue

## 2023-08-11

One of the few pieces of parenting advice that I‚Äôm highly confident in and think generalizes widely across personalities, family cultures, ages:

If your child is concentrating on doing something, don‚Äôt interrupt. Don‚Äôt correct, don‚Äôt compliment, don‚Äôt join in. Leave them be.

## 2023-08-09

üî•üî•üî•
Introducing the newest WizardLM-70B V1.0 model !

1. WizardLM-70B V1.0 achieves a substantial and comprehensive improvement on coding, mathematical reasoning and open-domain conversation capacities.

2. This model is license friendly, and follows the same license with Meta‚Ä¶ https://t.co/6H6DIcRt8E


## 2023-08-09

All the Diseases of Civilisation arise from the body's survival mechanisms being chronically engaged. Fat storage (visceral/subcutaneous), inflammation, blood glucose/insulin resistance.

This means that everyone is predisposed to get sick in an abundant calorific environment +‚Ä¶

## 2023-08-09

Sahil released a replit-3b with function calling. Data is open-source so you can see the kind of data Glaive can generate. https://t.co/7hXnroArmI

## 2023-08-08

A crucial component of building an LLM QA system is tuning the text chunk size üß±.

How do you evaluate which chunk size gives you the best retrieval/query performance, especially without human labels? ü§î

(mini-guide below üëá)

One trick here is to ensemble different strategies‚Ä¶ https://t.co/Y0pcHaVHWk

## 2023-08-08

There is a guide to help you understand how PayNym (private payments) works.  It will be widely used sooner than later.

credit to: @BitcoinQ_A 
üì∑
https://t.co/7gLCOX0K4B

## 2023-08-08

New https://t.co/9xi15J0h40 on Denoising Diffusion Models. Covers Langevin sampling, diffusion sampling on 1-D mixture of Gaussians (closed form) and denoising score matching with neural networks (in @PyTorch).
https://t.co/cLl3ihuOgK https://t.co/koW4lRJhCL

## 2023-08-08

The KV-cache for self-attention is (arguably) the most effective/simple technique for speeding up LLM inference. I'm going to post a more extensive explanation tomorrow, but this writeup I found is super clear and even provides an implementation!

https://t.co/3G5MijbhFg

## 2023-08-08

There are too many options for building information retrieval:
- Chunk size
- Query strategy (top-k, hybrid, MMR)

Idea: What if we ensembled *all of the options* + let an LLM prune the pooled results? üëá
‚úÖ More general retriever (though more üí∞)
‚úÖ Benchmark diff strategies https://t.co/9nEt92z4F4

## 2023-08-08

There are a plethora of metrics available about different aspects of the Bitcoin network. They can help us track its adoption and use. You can check out dozens of statistics sites linked from here: https://t.co/AntcXO0QOH

## 2023-08-08

The results are in!!! Learn about the top projects and the benefits of building in public #Ai4ALL 

* 173 Makers
*  44 Projects
* 8 incorporated NOSTR
* NIPs created (1 - nip 90)
* 23 Workshops 
* Replit prizes ($2,500) and bounties (11)
* Hackathon Prizes ($14,000) https://t.co/XBlDfadZJm

## 2023-08-08

Tip for better RAG systemsüí°: don‚Äôt just store raw text chunks, augment them with structured data.
‚úÖEnables metadata filtering
‚úÖHelps bias embeddings

Here‚Äôs a guide on how to use the @huggingface span-marker to extract entities for this exact purposeüìï: https://t.co/pSlLD9FHRA https://t.co/Gwwoeu3i9H

## 2023-08-07

Took some notes on how to serve Llama-70b with vLLM and @modal_labs, using distributed inference

https://t.co/RFD7FBbetH

## 2023-08-07

I just published a blog post about Lightning for those who haven‚Äôt checked in on it in a while. Huge thanks to @starkness, @RyanTheGentry, @davidmarcus, @stonecoldpat0, @therealkingonly, and @udiWertheimer for their feedback on it!

https://t.co/R8Bi152QAK

## 2023-08-07

The SpanMarker model (by tomaarsen on @huggingface) is incredible for NER üñãÔ∏è

Can use it on top of powerful LM‚Äôs like BERT/RoBERTa 

Use it to automate entity extraction in @llama_index -&gt; increase the retrieval performance of your RAG system! üöÄüëá

https://t.co/zb4caioX2M https://t.co/iykjJnLGol

## 2023-08-07

üéâüéâüéâ¬†Good news for open source community: Anyscale Endpoints (AE) is released; The fastest and most scalable API to integrate OSS LLMs in your applications.

Get immediate access: https://t.co/9375eLh6Ul

Blog: https://t.co/Gb24qpi8rG

üßµThread below 1/Nüëá

## 2023-08-07

Interestingly, DeBERTa-1.5B (and encoder-only model) beats Llama 2 on BoolQ, which is a nice example that encoders still outperform large decoders on classification task. 

For fairness: The DeBERTa-1.5B model was likely finetuned on the training data 

1/3 https://t.co/jo46ggr9Qr

## 2023-08-07

I simply take my qlora after one epoch, ask it one of the questions in the dataset, if incorrect, simply check wandb and wait for one more epoch and once it has grokked everything, just use one in the epoch before 

vibe based tuning

## 2023-08-06

I wrote about my process for publishing annotated versions of my talks - like https://t.co/0Dbkxz2BpI - and shared a new tool I built to help with that progress, allowing me to OCR my slides for alt= attributes and type up annotations for them in Markdown

https://t.co/1lfqMP1khD

## 2023-08-06

1. This is a thread about Quantum Mechanics and how we interpret it. The way QM is presented bothers me from time to time (at least when I have time to think about it). Indeed, time plays the key role in the whole issue.

## 2023-08-05

Bitcoin is Linux for money

## 2023-08-05

Use cases for encoder LLMs (classification) &amp; decoder LLMs (chatbots) are obvious.

Seq2seq tasks like translation, where it makes sense to have access to the whole input, is where it gets interesting.
Re encoder-decoder LLMs (eg T5): Is there still a benefit of using an encoder?

## 2023-08-05

Yann LeCun is obviously a legend but I found this tweet to be quite misinformed. 

The whole point of "emergent abilities" such as few-shot prompting and chain-of-thought prompting, is that we clearly *did not* explicitly train or fine-tune them into the model. These abilities‚Ä¶ https://t.co/toFsM0vJ24

## 2023-08-04

@AliYeysides Actually, this looks pretty promising https://t.co/LSkY5PdqRi

## 2023-08-04

Life either makes us humble, grateful, and generous, or grandiose, entitled, and envious.

Which road we travel is up to us.

## 2023-08-02

I wrote an overview of Gorilla (and similar models) for my newsletter. It is called "Language Models and Friends: Gorilla, HuggingGPT, TaskMatrix, and More". I can't post a direct link to my newsletter, but you can navigate to it via the link below!

https://t.co/iGWErsCmoD

## 2023-08-01

Functime is an interesting forecasting library.

‚Ä¢ Conformal predictions
‚Ä¢ Time-series embeddings
‚Ä¢ Polars for feature engineering
‚Ä¢ Several models (incl. LightGBM/XGBoost)

IMO it has a really nice API with solid and fast feature engineering.

But there are some downsides:

## 2023-07-31

I'm finding LM Studio to be the best local UX.

- Easy to Use
- Feature Rich
- Feels Good
- Supports Markdown
- Runs Hermes ;] https://t.co/1VMSBRkW8Q

## 2023-07-30

Big fan of https://t.co/DbHdVer9l7

This is how online services should be: No question, no account, pay in sats, do its job. Done.

## 2023-07-30

It's harder to learn from happy and successful people, because you don't constantly see them posting fake smiles on social media, or trying to sell you some product.

The kind of people you want to learn from, those who already have everything they want, are living private lives.

## 2023-07-30

This is really masterfully done. The subtitle might make it appear foreboding, but it assumes very little background, and is primarily focused on a computational perspective. 

h/t: Sameer Agarwal (@sandwichmaker), who recommended it to me, but doesn't hang around here anymore. https://t.co/RwoA0XrllX

## 2023-07-30

Finally! The long-awaited sequel in my series of blog posts: How does a Lie algebra encode a space?
https://t.co/gxabsQYnTO
1/4

## 2023-07-29

When you're under the fiat spell, you pursue things you don't really want to please people you don't really like to get things you don't really need.

Bitcoin breaks the fiat spell.

## 2023-07-24

Codegen2.5 is only 7B parameters and matches starcoder on benchmarks which is double the size 15B. Not only that but the architecture is llama based which makes it ideal for local code model fine tuning https://t.co/PsxYpLErEU

## 2023-07-24

I tried some structured extraction with llama2  via perplexitys web client 

Here are some qualitative results

1) llama7b can do data mining / extraction tasks easily

2) llama13b can do almost all tasks even table formatting ones easily. Except for reasoning about planing tasks‚Ä¶

## 2023-07-22

In LLM Chatbot project, you can inject "system prompt" as you like. 

Also, you can actually *see* the underlying prompt to the model. 

Like how many past conversations to take account, what prompt style is used (Alpaca, Vicuna, ...), and the system prompt. https://t.co/3VvdqpGNw5

## 2023-07-21

Have you run into this too? 

Empirically we've noticed this happens more for non-GPT-4 models (e.g. gpt-3.5-turbo).

Check out these learnings and others here: https://t.co/S9IqBy7dIe

## 2023-07-21

One of the biggest ironies in Data Science is that we call tabular data ‚Äústructured data‚Äù, while all other data modalities (text, audio, images, video, etc.) are considered ‚Äúunstructured data.‚Äù

1/5

## 2023-07-21

Introducing üéôÔ∏èDialogStudioüéôÔ∏è, the largest and most diverse dialogue dataset collection with diverse goals Ôºàe.g. task-oriented, open-domain, NLU, etc.) and different domains (e.g. finance, insurance software, movie, etc.)
https://t.co/t0alV5kztD 
https://t.co/JkpnKxvj7d
#NLP #AI

## 2023-07-20

If you‚Äôre building LLM apps over your data systems, there‚Äôs more uses to vector databases than you might think! üí°

Broke üö´: Using a vector db to directly index external data. Do top-k embedding lookup/retrieval.

Woke üôè: you already have access to a rich query language (e.g.‚Ä¶ https://t.co/ZhvC3lvWxb

## 2023-07-19

To carry your past mistakes, future worries and current responsibilities all at once is too much. A little self-forgiveness and faith help a lot.

## 2023-07-19

üî• üö® LLaMA2-70B just launched in @replicatehq
and added to our chat playground tooüí´:  

Try it here: https://t.co/1X5hvc2DWH

Replicate: https://t.co/Cb8boxFykI https://t.co/pDvrlM3Wde

## 2023-07-18

To do: Continue pretraining LLaMA V2 on starcoder's dataset to destroy ChatGPT-3.5 on EVERYTHING.

## 2023-07-18

TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT

Presents TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. 

https://t.co/iDkRiabJYp https://t.co/1HKsUy9ZM9

## 2023-07-17

üåü Quantization for everyone ! üöÄ

With the new release of accelerate 0.21.0, you can now quantize any model in 8bit or 4bit precision without sacrificing performance with ü§ó Accelerate and bitsandbytes library.

Documentation: https://t.co/4EOkQxrNZS

More details in the üßµ

## 2023-07-17

so much alpha in actually understanding how things work
the abstraction is begging to be broken
break it

## 2023-07-17

Paper is here https://t.co/3jsqJmAysE

## 2023-07-16

The strongest model you didn't know about:

- The best model.
- Trained on the best dataset.

OpenChat V2 x OpenOrca

OpenChat V2, A leading open-source model was fine-tuned last night on Open-Orca's data.

Link: https://t.co/ZgVDd97ctO https://t.co/jG9Abqeos0

## 2023-07-15

From long context lengths in LLMs to more efficient transformers for diffusion models. 
It's been another month with lots of interesting research. 

I put together an annotated list of 24 highlights from June to July: https://t.co/txpbYIFyOM

## 2023-07-15

Good Googlers: Leaving for Foundational large model startups

Bad Googlers: Leaving for Wrapper AI hackathon projects

Mid Googlers: Leaving engineering to become EM

Meanwhile sundar, watching some eastern euro dev create a better model than PaLM that runs on TI calculators: https://t.co/A9Xc4OAPTB

## 2023-07-14

Things I learned
- GPT3.5 is good at remixing words
- GPT4 actually understands things - it gets nuance, GPT3.5 not really.
- if you want anything task specific you should probably train ur own model
- you can label almost 1k in an hour if you're fast
- get a good labeling script

## 2023-07-14

There‚Äôs been two main stacks for LLM-based QA over data:
- Unstructured: vector db + RAG
- Structured: Text-to-SQL 

We now have full native support for a THIRD stack (s/o @wey_gu): build knowledge graph w/ graph db, then query with text-to-Cypher! üï∏Ô∏èüîé

https://t.co/VG4V5ekvJe https://t.co/ImiakVqrXK

## 2023-07-13

I love how we can get a massive model to educate a smaller one. The smaller model is still pretty dumb outside of the specified task. But you get faster inference, cheaper models, cheaper training. Do you really need complex reasoning to summarize an article? I think not

## 2023-07-13

Paper is here https://t.co/LffJ9utsyy

## 2023-07-13

ÁôæÊ®°Â§ßÊàò üîõ  The battle of 100 large models

A trendy word in Chinese large model land. It gives you an idea about how this business is growing in China. üöÄ

Show case or business case? üßµ https://t.co/fVekKgN779

## 2023-07-13

Question answer models can work well at 13B and 7B sizes when fine tuned. This is because generally you would have them answer questions related to your documentation or whatever knowledge base you built up

## 2023-07-12

We have released an Orca model based on 200,000 sampling of GPT-4 Orca set. 

By @alignment_lab, @winglian, AutoMeta, NeverendingToast, Entropi, @Teknium1 &amp; Others

Download here: https://t.co/2cgSzIUT9e

Benchmarks pictured as well as picture of an orca and a @nomic_ai atlas ^_^ https://t.co/tb9bP0C3q0

## 2023-07-11

3/ Blog post with more in-depth learning: https://t.co/aPpwUnoabx

## 2023-07-11

Don‚Äôt get distracted by gpt4 architecture. Unless you have the compute (money) ready and a team with experience, there‚Äôs little reason to chase what they are doing. You can get very good results from smaller models, using more recent advancements like DPO, a replacement for rlhf

## 2023-07-11

@sudonymously @Yampeleg https://t.co/7Vq1JE7aku

## 2023-07-10

we need more full-stack academics

## 2023-07-09

Re: langchain, I think it‚Äôs underrated value is like a cookbook of sorts:

- templates on how to implement patterns CoT, reader-retrieval, ReAct etc (code is often easier to understand than papers)
- code that allows you to extract text from any application (these are modular)
-‚Ä¶

## 2023-07-08

Topological Data Analysis as you've never seen it before: @robertghrist is bringing my lecture notes (https://t.co/gqZwXbwuOG) to life in a new animated video series (https://t.co/k6YJaPFx28)!

Here are the videos which have been released so far... enjoy! And tell your friends.

## 2023-07-08

Confirmed Context Windows:

GPT-4 + Code interpreter: 8K ctx_window
GPT-4 + Plugins: 8K ctx_window
GPT-4 Original: 4K ctx_window.

## 2023-07-08

Enjoy the origin story of Wizard-Vicuna-13B-Uncensored-SuperHOT-8k-GGML models.

After this you can also pretend like me to be an AI enthusiast on /lmg https://t.co/2jXtIGJaNY

## 2023-07-08

Fundamentally, the reason some managers dislike work-from-home is that they're unable to evaluate employee productivity other than by measuring hours spent in a chair in the office. If you can judge people's actual output accurately, unlimited WFH is simply a better policy.

## 2023-07-07

Salesforce really pushing out some good open models. The fact that this one is a drop in for llama models is pretty based because it can do code and outperforms the actual OpenLLaMA project https://t.co/ySReZCBWuH

## 2023-07-06

Lightning Service Providers are the key to unlocking instant #bitcoin payments, eliminating the need for custodial wallets. LSPs offer convenience without any compromises.

Learn more:

https://t.co/kkUXt0P8c5

## 2023-07-06

Most post processing of LLMs can be solved by simply fine tuning the model to output what you expect. I don't really think you need complex frameworks or wrappers. Example get the model to generate json by fine tuning it on 500-1000 samples. It doesn't get any easier than this

## 2023-07-06

Flacuna: Unleashing the Problem Solving Power of Vicuna using FLAN Fine-Tuning

paper page: https://t.co/SN6VCtGeau

Recently, the release of INSTRUCTEVAL has provided valuable insights into the performance of large language models (LLMs) that utilize encoder-decoder or‚Ä¶ https://t.co/ex5m5awplx

## 2023-07-06

If you‚Äôre building ‚Äúchat over your PDFs‚Äù with LLMs, you need to deal with the pesky issue of how to parse embedded tables/diagrams.

Native text splitting + top-k on your tables == bad results! ‚õîÔ∏èüö´

A nuanced, hierarchical data representation over your PDF can help üëá https://t.co/tU6lS9GSzL

## 2023-07-05

One of the cool use cases we've seen recently for Lightning is @dannydiekroeger's Deezy AI. 

It's a wrapper for ChatGPT that allows you to pay for individual prompts with Lightning. This opens up cool AI tools to a big group of people who might not be able to access it before,‚Ä¶ https://t.co/QgnnQ3UTxI

## 2023-07-05

curious about nix + bitcoin?? check out this op-ed i wrote for @BitcoinMagazine about the project (and don‚Äôt forget to come to @btcplusplus this Oct in Berlin!)

https://t.co/s3aH3VqW6p

## 2023-07-05

üî• Major Updates on the LLM Survey üî•

* 34+ new pages, 200+ new references
* New figures (e.g. ü¶ôLLaMA family)
* New chapters (e.g. complex task planning w/ LLMs)
* 26 useful prompting tips
* Empirical evaluation of 8 abilities of LLMs with specially selected tasks

1/n üßµ https://t.co/NjFoY39zUw

## 2023-07-05

Talked to people in government and they spoke of horror stories stemming from ChatGPT hallucinations making it into official documents. Showed them @perplexity_ai and I don‚Äôt think they‚Äôll be using any other AI system from now on.

## 2023-07-03

Bitcoin payments made easy with @Replit and @zebedeeio ‚ö°Ô∏è

‚ÄúWe are at a stage in the adoption of this tech where devs can add support for global instant payments with just 3 lines of code.‚Äù ü§ô

ZEBEDEE lets you move money at the Speed of the Internet! ü´µ

https://t.co/eJY7AYvxC7

## 2023-07-03

Tokenization and decoding are two weakest points of current LLMs. 

Current tokenization methods like BPE are poor man‚Äôs solution and can‚Äôt scale to true multi-modal. 

Similarly, current decoding methods such as beam search are poor man‚Äôs replacement for the proper planning‚Ä¶

## 2023-07-03

AI is strong today in bounded domains - coding assist, driver assist, translation, transcription, and stock art.

## 2023-07-03

Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting

Proposes PRP, which uses the query and a pair of documents as the prompt for LLMs to perform ranking tasks.

https://t.co/sc6f0rMOB5 https://t.co/WNM7bAjJlh

## 2023-07-02

Quick note ‚Äì we've transitioned from the deprecated vicuna benchmark to a more advanced MT-bench, including more challenging tasks and addressing biases/limitations in gpt4 eval.

We find OpenChat's performance on MT-bench is similar to wizardlm-13b.
That's said, there remains a‚Ä¶ https://t.co/f7tpZ6VKAM

## 2023-07-01

We have a competitor to @huggingface text-generation-inference:

vLLM üî•

‚Ä¢ Claims to be 3.5x faster
‚Ä¢ Custom CUDA kernels
‚Ä¢ Uses PagedAttention for memory efficiency

Looks like another interesting option with a decent Python interface! https://t.co/NEdxAVE3n9

## 2023-06-30

Recent research on language models has aimed to increase the maximum allowable context length of the underlying model. But, how can we enable an LLM to handle longer inputs? One way is through the use of ALiBi‚Ä¶

Vanilla position embeddings. Decoder-only transformer architectures‚Ä¶ https://t.co/LVCPeFvUQc

## 2023-06-30

If you are curious how full fine-tune vs qlora fine-tuned model performs, I've just added qlora versions of the airoboros 13b and 7b 1.4 models for comparison.  Let me know what you find!
https://t.co/qViYVbg1pa
https://t.co/dKuS8TWVjI

## 2023-06-29

In a production setting, your LLM app needs a sophisticated data pipeline that can handle inserts and updates. ‚ôªÔ∏è

@wenqi_glantz has written a *fantastic* tutorial üìó on how to use the advanced data management features in LlamaIndex to build this! ‚≠êÔ∏è

üëâ https://t.co/yDNk4TI8KF

## 2023-06-28

The MPT suite of large language models (LLMs) by MosaicML has become incredibly popular. But, what makes these models so special? Although there are a variety of reasons for the popularity of MPT, I find these models to be especially useful due to a few unique components‚Ä¶

Fully‚Ä¶ https://t.co/DwFJpAIbaS

## 2023-06-28

üéâNew paper alert! üéâ
How can we learn features that do not directly contribute to reducing your loss?

We've just released a new paper where we tackle this crucial problem in representation learning  üßµ
https://t.co/0QaLKMCoWx

## 2023-06-28

This article perfectly illustrates the point that I have been making for a while: we are now conflating the current ‚Äòproper‚Äô AI tools, such as GenerativeAI, chatbots, LLMs and all that, with what has now for years been considered ‚ÄòAI‚Äô, such as ML-based predictive modeling. And‚Ä¶ https://t.co/ugtrhf4Zhc

## 2023-06-27

Dust uses large language models on internal data to improve team productivity https://t.co/cbXNHGOSkn by @romaindillet

## 2023-06-25

Principal component analysis and non-negative matrix factorization are two particular of non-convex matrix factorization problem. They differ by the constraints on the atoms D and coefficients X. https://t.co/TAWN787ofy https://t.co/knuRHQNTDe https://t.co/bgZ1RH2fqK

## 2023-06-23

üßµWhen using #LoRA it is important to apply it to ALL `Linear` layers of the model to get similar results to "full fine-tuning."  If you are using @huggingface/peft library, you will see some LoRA configs like the following https://t.co/Vb1TIMKjMq

## 2023-06-23

modern grug stack

vpn -&gt; tailscale
hosting -&gt; $5/mo vps
reverse proxy -&gt; nginx
db -&gt; sqlite in prod
backend -&gt; go
frontend -&gt; htmx + tailwind
ci -&gt; git init --bare + hooks + scp

## 2023-06-23

My next open source stack: 
API - @FastAPI  * @modal_labs 
Frontend - @nextjs 
LLM - @LangChainAI @OpenAI 
Vector DB - @pinecone 
Database - @SnowflakeDB 
Hosting - @vercel 
Website analytics - @umami_software 
Rate limiting - @upstash 
Auth - @nextauthjs 
Css - @tailwindcss

## 2023-06-19

Receiver Operating Characteristic (ROC) got its name in WWII from Radar, invented to detect enemy aircraft and ships.

ROC curves show true positive rate vs false positive rate, parametrized by a detection threshold. 

a small thread

1/n

animation by @dariyasydykova https://t.co/a2MAdf516R

## 2023-06-18

After a long tour in Greece and Italy, finally find some time to post slides of my lectures at ACDL 2023 online at: https://t.co/mvzwWpYUaE This is the most complete set of lectures on white-box and closed-loop deep networks. But I have yet to find time to record them as videos.

## 2023-06-17

I don't agree with focusing the mind in the pursuit of difficult tasks. Attempts to resolve difficulty don't benefit by insulating our efforts from the vagaries of reality. 

Severing our struggle from natural environments is a vestige of the education system, not the dynamics of‚Ä¶

## 2023-06-16

How physical qualities degrade as you age.

I've been fit all my life. From late twenties to fifty I trained competitively. *In my experience* here's the order in which physical performance degrades.

1 Recovery takes longer. You can train just as hard but are achy for longer.‚Ä¶ https://t.co/SUQz7kcWHU

## 2023-06-16

I like this paper. They prove that transformers are guaranteed to suffer from compounding errors when doing long reasoning chains (as @ylecun  has argued), and much apparent "success" is just due to unreliable pattern matching / shortcut learning.
https://t.co/ej3nqeQ5KR

## 2023-06-15

Solomon often offers what appears to be a mere hodgepodge of thought but turns out to be brilliant gems of wisdom. #nottobemissed @RabbiWolpe @Rabbishish @RabbiPoupko @Ravbaruch @Torahtech613 @Daroff @austinrabbi @RabbiBetcher @JasonBedrick @LoewyLawFirm  https://t.co/ZtPLq5yM18

## 2023-06-15

Information Geometry is about dual geometry:

Build your own information geometry by choosing 
(1) manifold M, 
(2) metric g
(3) affine connection ‚àá

Dual connection is simply ‚àá*=2‚àá^g- ‚àá

Tutorial without prerequisite on differential geometry:

üëâhttps://t.co/Qxd6BWEvN1 https://t.co/HamfZIYlGa

## 2023-06-14

Things everybody wants:

1) living in a place where they feel respected and accepted

2) a home that offers stability and safety

3) a loving family and loyal friends who care

4) enough money to not worry about food, rent, medical emergencies

5) enough free time to be able to‚Ä¶

## 2023-06-14

How to diagnose a mysterious process that‚Äôs taking too much CPU, memory, IO, etc?

The diagram below illustrates helpful tools in a Linux system.

üîπ‚Äòvmstat‚Äô - reports information about processes, memory, paging, block IO, traps, and CPU activity.
üîπ‚Äòiostat‚Äô - reports CPU and‚Ä¶ https://t.co/38VqTHaLWs

## 2023-06-13

Increasingly feeling a core schism in the emerging AI Engineering community. in my head, it's either:

- LLM Core, Code Shell (well established)
- LLM Shell, Code Core (emerging)

I see a future where the LLM in the loop is unbundled and dedicated/optimized to do one thing well.‚Ä¶ https://t.co/JMdCLva1nv

## 2023-06-13

üéâWow!üéâ Our paper 'Topological Singularity Detection At Multiple Scales' was accepted at #ICML2023.

üëâWe question the manifold hypothesis &amp; develop a method to detect singularities, i.e. points that violate this 'manifoldness' assumption.

#topology #MachineLearning 

üßµ1/n https://t.co/bSFbzrTS1j

## 2023-06-13

There is, in the best of our moments, a detached feeling of accepting what nature shows us. 

Not "skill" or  "knowledge" but our capitulation. A moment when all your wrestling to understand gives way to the apparent. 

When the hubris of enlightenment collapses under the weight‚Ä¶

## 2023-06-12

Bitcoin ‚úÖ 
Tor ‚úÖ 
Fulcrum ‚úÖ
Dojo ‚úÖ
Mempool ‚úÖ
Whirlpool ‚úÖ

x86 full node setup guide.

https://t.co/hGHZBMYoqu

## 2023-06-12

I'm totally serious. If the job was done really well and I had say a +99% satisfaction rate with all the answers as long as the questions are genuine questions and not purposefully trick questions looking to fool the AI, I would pay $10,000 for this. 

My own customer support bot

## 2023-06-12

Time-series forecasting is becoming a lost art.

It‚Äôs the engine behind the $5,000,000,000,000 retail industry in the US.

And yet, research progress is slow and the best tricks are locked away.

But why is it particularly tough even in the current AI breakthrough?

Here‚Äôs why üßµ

## 2023-06-12

Can Large Language Models Infer Causation from Correlation?

paper page: https://t.co/Pzl3iuLYe6
dataset: https://t.co/luva7VqVdh

Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years,‚Ä¶ https://t.co/zaB9OvDTBI

## 2023-06-11

This clearly written article by @gmusser explains how notions from computational complexity theory have advanced our understanding of quantum gravity and black holes. The video is good, too.
https://t.co/w096xm3Nkg

## 2023-06-09

Overthinking less, a thread for fellow introverts.

## 2023-06-08

Marc Andreessen recently tweeted ‚ÄúIn our new era of AI: Every child will have an A.I. tutor that is infinitely patient, infinitely knowledgeable, infinitely helpful.‚Äù

That era is here. 

Try it yourself üëâ https://t.co/TZprbulgWx

1/n https://t.co/XigK2rwvD4

## 2023-06-08

On a long enough time frame, everyone either contributes honorably and then disappears...or becomes a delusional, self-righteous narcissist and then disappears.

## 2023-06-08

There are only 2 steps to life:

1) Understanding what to do
2) Doing it

The divide between people is based on whether:

- You never do either
- Get the 1st wrong so the 2nd doesn't matter
- Do the 1st but never the 2nd
- Do the 1st but the 2nd inconsistently
- Do them both‚Ä¶

## 2023-06-08

By 2025 AIs invoking the Python interpreter will outnumber humans doing the same.

## 2023-06-06

Bitcoin isn't a security.
Bitcoin is security.

## 2023-06-05

There are unpleasant things you have to do when self-employed, but you will never need to be a Tom Wambsgans.

https://t.co/vdJivgHGUz https://t.co/O2pN5Y3PSr

## 2023-06-05

To truly and fully bridge the gap between theory and practice, we also release the code on the GitHub repo: https://t.co/cpAUDpWm2y . AI is still at its infancy, quite contrary to what many want you to be afraid. Opening (source) everything as white boxes is the way to progress.

## 2023-06-04

Comparison of the Wasserstein, Hellinger, Kullback-Leibler and reverse KL on the space of Gaussian distributions. https://t.co/QLbZdoosP4 https://t.co/BRLXo0lRPF

## 2023-06-03

In machine learning, each bug is a gift. Once discovered, each idea that "didn't work" before has a chance of working; behavior that was difficult to understand may become obvious. Overall, a bugfix is a moment to re-examine what we thought we knew, for the better.

## 2023-06-02

ADHD is probably not a disorder. Important mental model: https://t.co/z22tMBmhHo

## 2023-05-31

Some usual tricks of the trade in Machine Learning.
     Which recent ones is missing? https://t.co/ZesdTqCsIW

## 2023-05-30

Below is current model ranking after extensive experiment on complex reasoning tasks 

Code at https://t.co/WA30rW5fQ7
Paper https://t.co/Yqxguf01TA https://t.co/kKGR5rOrC8

## 2023-05-29

Why write an essay when you can type a few words and have AI generate one for you? Why write an email when AI can auto-respond for you with all the typical pleasantries and talking points?

While AI doing these things for you is likely to happen, it‚Äôs not necessarily a good‚Ä¶

## 2023-05-28

A fair snapshot at the difference between modern education and classical education. https://t.co/jYxUBt46aS

## 2023-05-24

The good people at @brexHQ published a great guide to prompting!

Going to thread some highlights below, but make sure to check out the full guide: https://t.co/sAr3Guvjwj

Read on for increasingly sophisticated prompt techniques:

## 2023-05-24

my biggest speed constraint in development is deliberating over the best way to do something

I'm learning to just ignore the instinct to deliberate and freeze in analysis paralysis: go on gut feeling. If its wrong, that just means that I've learned something

## 2023-05-24

ü§ñLLMs (autoregressive transformers) are not designed to reason in non-linear structures like humans... 
ü§îInstead of using it as a straight-minded reasoner, what about regarding it as a world modelüåéand planning with it?
Check out our new work: Reasoning via Planning (RAP) üßµ https://t.co/atmsFgA4w7

## 2023-05-23

I enjoyed this post by @johnrobinsn - nice end to end practical example on fine tuning OSS LLMs on consumer hardware.  Uses LoRA, OpenLlama, etc.

If you are interested in fine-tuning LLMs will take you less than an hour to get started w/this example

https://t.co/ZmeJvXCKcv

## 2023-05-23

After checking out https://t.co/HU7NlekVmu, I'm not sure I'll ever use Google Scholar again.  It's basically an AI-enhanced Google Scholar, where you can ask natural language research questions, and get a summary/interpretation along with the matching research papers.

## 2023-05-23

The great leaders in the Torah were reluctant to lead: Moses, Jeremiah, Samuel.  Thinking you weren‚Äôt worthy of the job was part of the qualification for the job because it meant to understood the enormity of the responsibility and were prepared to confront your own limitations.

## 2023-05-22

Curating high-quality posts from AI Twitter with my own take, Vol. 2

No breaking news, no productivity hack, no insane moments. Just some solid stuff that makes AI a bit better than last week.

Time for Chef's pick. Here we go: https://t.co/KkVE1B8szK

## 2023-05-22

LIMA :
LLaMA 65B + 1000 supervised samples = {GPT4, Bard} level performance.

From @MetaAI

https://t.co/FIuIo6agXa

## 2023-05-20

3 obstacles for a piece of content to have an impact on people.
1. Production
2. Dissemination
3. Attention

Computers &amp; networks have made production &amp; dissemination easy.
The bottleneck is now capturing the audience's attention.

Generative AI increases 1, but not 2 nor 3.

## 2023-05-20

Community: Eval for LLMs are broken! Academic benchmarks are not representative of real world performance! üôÖ‚Äç‚ôÇÔ∏è. We need better evals!

Also the same community: Lets make definitive rankings &amp; leaderboards based on just four zero-shot "LM harness" tasks!ü§∑‚Äç‚ôÇÔ∏èü§∑‚Äç‚ôÇÔ∏è

Not wanting to single‚Ä¶

## 2023-05-19

This has gotta be the most profound thing I've ever heard

The 3 great theories of 20th century physics.. are the interplay between computational irreducibility and the computational boundedness of observers.. All are derivable but not just from mathematics.. they require that‚Ä¶ https://t.co/M2KQ2C4tyi

## 2023-05-19

In the era of neural scaling laws and "emergent" capabilities of AI, why don't we see more plots like this one to make the points about the emergence and scaling better justified and founded?!

## 2023-05-19

Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold

paper page: https://t.co/Gjcm1smqfl https://t.co/XHQIiMdYOA

## 2023-05-19

New book on 

"Information Theory: From Coding to Learning" 

coming out soon: 600+ pages with 150+ exercises
by Y. Polyanskiy and Y. Wu 
Cambridge University Press

Check table of contents and download book draft PDF from 

üëâhttps://t.co/M9g5AAuAK4 https://t.co/70fg2vFg0p

## 2023-05-18

Here is our ‚Äúslick‚Äù RLHF-alternative without RL: https://t.co/yhyH2cjNec (SLiC-HF)

TL;DR: Works as well as RLHF, but a lot simpler.

About as easy and efficient as fine-tuning. Much better than simply fine-tuning on good examples.

From great collaborators: @yaozhaoai,‚Ä¶ https://t.co/ZlA0vMUpAH

## 2023-05-16

Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation

abs: https://t.co/w7RwaA0mic
paper page: https://t.co/rudQPxB662 https://t.co/0bTZfLGDaX

## 2023-05-15

Using bijection between squared Euclidean distance and isotropic Gaussians: 
p(x,Œº) ‚àù exp(-‚Äñx-Œº‚Äñ¬≤)
interpret k-means as a learning algorithm for mixtures of isotropic Gaussians: A Classification Expectation Maximization (CEM)
Extends to Bregman k-means

üëâhttps://t.co/5dfarHrOdN https://t.co/YXl4klGK1t

## 2023-05-15

6 months ago, in my notion about GPT evolution, we had a hypothesis: code improves reasoning

Now we see the bloom of great papers, open source projects, insightful blogs, prompt engineering techniques üéâüéâ

We also continuously explore on
https://t.co/WA30rW5NFF

Stay tuned!

## 2023-05-15

Promising. Everyone should hope that we can throw away tokenization in LLMs. Doing so naively creates (byte-level) sequences that are too long, so the devil is in the details.

Tokenization means that LLMs are not actually fully end-to-end. There is a whole separate stage with‚Ä¶

## 2023-05-15

Unpopular opinion, but I think it's way more difficult to use Generative AI in an area where you are a rank beginner than in a field where you have some subject matter expertise in.

For example, you can ask GPT-4 to build you a front end, but if you don't know anything about‚Ä¶

## 2023-05-13

The quest for social status and popularity as an end in itself is fun as hell...for a while...but it ultimately wrecks even the brightest minds.

## 2023-05-12

Llama and many recent open-source models have a significant architectural limitation

They use multi-head attention instead of multi-query attention (which is used by PaLM and probs Claude 100K)

This can result in slowdowns of up to 30x

Heres the math behind why (1/n) https://t.co/By7k1OOpSd

## 2023-05-09

RE: "how often do you see teams actually fine tuning LLMs?"
It's an interesting question, about how prompting (optimization over prefix tokens) and finetuning (optimization over weights) will be used over time. If people have data points please pitch in. 
I expect that finetuning‚Ä¶

## 2023-05-06

Oops haven't tweeted too much recently; I'm mostly watching with interest the open source LLM ecosystem experiencing early signs of a cambrian explosion. Roughly speaking the story as of now:

1. Pretraining LLM base models remains very expensive. Think: supercomputer + months.‚Ä¶

## 2023-05-05

Frequentist statistics is essential to how we make decisions and how we do science.

But the fundamental interpretation of frequentist statistics is ... weird

It took me years to fully grasp it.

Join me on a journey involving imagined experiments that help us make decisions:

## 2023-05-05

Everybody needs to check this out!!  One of the most thoughtful and comprehensive tools to apply machine learning for scientific discovery.  Really a tour de force

## 2023-05-04

In this short note, we explore thinking of the traditional idea of "distributed representations" as two distinct phenomena: "composition" and "superposition". We walk through toy examples from Thorpe (1989), discussing them from this lens.

https://t.co/EXobjAH00m https://t.co/zTnQ9V4g6E

## 2023-05-04

22 lessons you need to teach your children:

1. You need to teach your children that they aren't the victim.

2. You need to teach your children that hard work will always beat talent.

## 2023-05-04

Always believe low-dim manifolds are the right underlying models for high-dim data distributions. The key to all (deep) representation learning is to identify and transform the manifolds properly. Here is our latest attempt to realize this computationally: https://t.co/h1Yy90d48W

## 2023-05-03

The core differences between GPT4 and 3.5 is the ability to perform complex tasks. In this post, we present a complete roadmap towards LLMs complex reasoning abilities, covering the full development stages: pretraining, SFT, RL, CoT prompting, and eval. https://t.co/1rzx02LNlx

## 2023-05-02

Diffusion Models are powerful for decision making:

üí° Provide an alternative to RL by stitching trajectories without dynamic programming!

üí°Compose skills without hand-design of heirarchies or task-planners!!

Presenting Decision Diffusers at #ICLR2023: https://t.co/p7Z2XtQZ87 https://t.co/iKmyVTomou

## 2023-05-02

True wealth is a combination of health, money, time, love, and peace of mind. You are wealthy once you find the right balance for you.

## 2023-05-01

It is interesting that, after a sober discussion of how AI works, Wolfram‚Äôs guess about why ChatGPT was suddenly so much more capable than expected is that, as the scale of the model increased, it found the hidden deep structural patterns of human thought. https://t.co/7LY5893IWs https://t.co/ygxmRbEmVZ

## 2023-04-30

Lightning will eventually scale to billions of users.

But not without the help of LSPs.

LSP THREAD: https://t.co/hZzsBgarGZ

## 2023-04-30

The CCP is Marxist in the way that the Mafia is Catholic. It's more like the ancient Mandarin bureaucracy than the Soviets. China has never had a civil society in the Western sense: There is family and there is state, a private and public sphere--more like Sicily than Sweden.

## 2023-04-30

What are the methods that can 100x reduce LLM inference costs? Smaller models? Quantization? Early exist? Non-autoregressive generation? What else? ü§îü§î

## 2023-04-28

@ylecun and I have been pondering the concept of optimal representation in self-supervised learning, and we're excited to share our findings in a recently published paper! üìùüîç https://t.co/FOtXOoXs19

## 2023-04-27

You should expect your biggest accomplishments to materialize all at once, preceded by a very long period of opaque progress.

## 2023-04-24

Tools are lined up to help onboard people to earning BTC but they haven't been deployed yet. LNC allows you to connect browsers to LND. Voltage allows you to deploy LND. The Bitcoin Company can convert LN to Visa. You could earn and spend without even needing to know you used LN.

## 2023-04-24

TiDE: Time-series Dense Encoder

Claims to be 5-10x faster (and 6% more accurate) than Transformers (PatchTST) with much longer history due to linear scaling.

It‚Äôs an MLP-based encoder-decoder architecture that leverages a custom distribution loss. üëá https://t.co/L4hGitZkxN

## 2023-04-22

Machine learning engineering is primarily about patience, attention to detail, and thinking deeply about small things. The day-to-day can be quite tedious &amp; frustrating ‚Äî¬†but the results of proper execution make it worth it.

## 2023-04-21

Topological Deep Learning is an immensely powerful and fast emerging field. Our new literature review https://t.co/MLp4AaV8vf is out and here‚Äôs why I‚Äôm very excited about itüßµ1/5

## 2023-04-20

The perpetually undervalued least-squares:

min‚Çì‚Äñy‚àíAx‚Äñ¬≤

can teach a lot about some complex ideas in modern machine learning including overfitting &amp; double-descent.

Let's assume A is n-by-p. So we have n data points and p parameters

1/n https://t.co/ot4or8rTgl

## 2023-04-18

Jupyter Notebooks are really handy working with Data Science and Machine Learning Projects.

Here are some really amazing and surprising things you can do with Jupyter Notebooks: https://t.co/FF726jPj4r

## 2023-04-14

Contractive autoencoders are a regularised energy-based model that limits low-energy regions by penalising the Jacobian squared Frobenius norm (its singular-values vector squared norm).
Thanks to @gabrielpeyre, I can show the sources and sinks of the dynamics field. https://t.co/yIJsdqxxCu

## 2023-04-13

some vectorstore analysis - what do folks like when building for production? https://t.co/NndjPlfGD7

## 2023-04-08

Don‚Äôt see it as work to be completed, see it as practice for your process.

## 2023-04-07

Flexibility, like strength, is an innate quality - you can improve it but some individuals will always be more flexible than others.

It's not lack of flexibility per se which is associated with aging, but the LOSS of flexibility relative to you. This is the same with gait speed,‚Ä¶

## 2023-03-31

"America you big imperialist bully" (#ÁæéÂ∏ù‰Ω†ÈÄôÂ§ßÊµÅÊ∞ì), a 1960 Chinese propaganda song once popular during the height of the Cold War, is making a comeback on Chinese internet lately, now that US-China relations is getting colder day by day. https://t.co/p2HUQlGPhu

## 2023-03-30

No technology or tool can fix people that have profound ethics problems.

I should know. I have all the technology and tools of this society totally available to me...and I am still totally incorregible!

## 2023-03-28

5. Retail &amp; Lifestyle

- Virtual try-on
- Supply chain optimization
- Visual search: allowing customers to search products based on images
- Personalized shopping experiences
- Predictive analytics for demand forecasting, cost optimization &amp; inventory management

## 2023-03-27

Many people dismiss concerns about the harms of new tech by saying the benefits outweigh the harms. But that's a dubious moral framework. Besides, it's the wrong question. The counterfactual isn't a world without tech, it's one where tech is developed and released responsibly.

## 2023-03-27

üòØ The greatest fear that drives people away from causality is not about complexity

It's about something much simpler

üßµ (1/n)

#causality #causalinference #python #machinelearning #datascience #casualtwitter https://t.co/PalrjbifY1

## 2023-03-25

My ML modeling trinity:

1. Tabular: XGBoost/LightGBM
2. Text: @huggingface Transformers
3. Vision: @wightmanr's timm

I can solve 95% of my problems with these libraries.

The rest is ChatGPT üòú

## 2023-03-25

How do you train a RecSys model and put it in production?

‚úÖ overcoming extreme cold start problem
‚úÖ serving real-time session-based recommendations using Merlin and Triton
‚úÖ recommendation click rate up to 170% vs control

Please find the talk here: https://t.co/pCZyTVcTkW https://t.co/FrRPgjgkfR

## 2023-03-24

How to train your own ControlNet? ü•Ö

We wrote a guide, ranging from deciding which controls to use üéõÔ∏è, how to prepare your dataset, all the way to gpus going brrr üî• 
(with an unexpected trip to the uncanny valley üëÄ)

From me and @pcuenq with ‚ù§Ô∏è
https://t.co/5LosE2R7lT

## 2023-03-24

So if you were made to feel like your ADHD is a character flaw remember we get strengths from how our brains are wired. We are creative, high energy, likely to have senses of humor, intuitive, enthusiastic, and our hyper focus allows for deep knowledge and expertise in many areas

## 2023-03-23

There are two visions for how people will interact with AI: putting AI into apps, and putting apps into AI. 

If the latter takes off:
‚ÄìLLMs are a kind of OS (foretold in ‚ÄúHer‚Äù).
‚ÄìBiggest user interface change since the GUI?
‚ÄìApp makers‚Äô fortunes controlled by a new middleman.

## 2023-03-20

PaLM is a really interesting decoder-style language model that I initially kind of ignored when it was published last year: https://t.co/R5tVuKYkSw

Turns out PaLM has 7 interesting architecture improvements over GPT.

1/9

## 2023-03-17

Language User Interfaces (LUIs) are the future.

Here are some patterns we know and love -- and some new ideas!

üåÄ Auto-Complete (Copilot)
üåÄ One-on-one Chat (ChatGPT)
üåÄ Command Palette (Replit Ghostwriter)
üí° Command Suggestion
üí° Multi-player Chat
üí° GitHub UX

Some examples:

## 2023-03-17

üòØ Confused about causality in Python?

Here's my take on the Python's causal ecosystem

üßµ (1/n)

#causality #causalinference #causaldiscovery #causaltwitter #python #machinelearning #datascience https://t.co/VaYfCcwPBK

## 2023-03-16

This might be the most eventful week AI has ever seen:

Monday:
-Stanford Alpaca 7B

Tuesday:
-GPT4
-Anthropic releases Claude
-Google's PaLM API
-AdeptAI raises $350M
-Google adds GenAI to workspaces

Wednesday: 
-Pytorch 2.0
-MidjourneyV5

Thursday:
-Microsoft 365 Copilot

## 2023-03-16

thread on my "frequency domain embedding" concept  via wave computing

example using a standing wave, such as a plucked string:

if a few overtones (110hz, 220hz, 330hz, ...) are superposed, they're all harmonic &amp; you get a simple repeating pattern that reveals a shape outline https://t.co/d3sDCHcPQO

## 2023-03-13

Someday, we will look back and realize that there were only two banks all along.

The Federal Reserve, the fully centralized bank, and all of its branches, and‚Ä¶

Bitcoin, the fully decentralized bank, and all of its stepchildren.

## 2023-03-10

You will get to an age when ALL your 'unexpected' health events will be negative.

What you need to do NOW is resist the comfort and and mindless calorific consumption of modernity - replace them with fasting, resistance training, aerobic  and anaerobic endurance - a‚Ä¶

## 2023-03-09

Image-to-image models have been called 'filters' since the early days of comp vision/imaging. But what does it mean to filter an image? 

If we choose some set of weights and apply them to the input image, what loss/objective function does this process optimize (if any)?

1/8 https://t.co/UOIZpeGzH5

## 2023-03-08

Foundation Models Powering Agents

Reviews connecting pre-trained models (eg LLMs) to decisions via:
-conditional generative modeling
-reinforcement learning
-optimal control
-prompting
-planning

-Models can drive decisions or serve world dynamics

Paper: https://t.co/Tv1bJPmgVG https://t.co/GqaDxJukaH

## 2023-03-07

Two of the hardest things about cognitively demanding work are:

1) You can‚Äôt really make it go faster. There are tricks to getting unstuck but it usually proceeds very nonlinearly. 

2) It often looks like it could have been done in a far shorter amount of time when it‚Äôs over.

## 2023-03-07

What do polar coordinates, polar matrix factorization, &amp; Helmholz decomposition of a vector field have in common? 

They‚Äôre all implied by Brenier‚Äôs Theorem: a cornerstone of Optimal Transport theory. It‚Äôs a fundamental decomposition result &amp; deserves to be better known.

1/5 https://t.co/DqtoBqOiLR

## 2023-03-05

Blog: https://t.co/GRiTqmWoeS
Model (HF): https://t.co/C05wo0jzJj
GitHub: https://t.co/VwlVhsOCeo
Paper (UL2): https://t.co/tWrgQjJR8x
Paper (Flan2): https://t.co/WZvts6SI93

## 2023-03-03

Are you rushing? https://t.co/hRAvLd1fNN

## 2023-03-03

ChatGPT (gpt-3.5-turbo) works best for conversational tasks, InstructGPT (text-davinci-003) for zero-shot, and Codex (Code-DaVinci-002) for few-shot in-context learning. 

As an NLP researcher, I am still valuable today, at least as a three-way classifier.

## 2023-03-03

Meet privately.

https://t.co/4dzSg11asO requires no e-mail, no phone number, no ID and it‚Äôs never on a server. 

Just simple üçêPeer-to-Peer Chats https://t.co/bLQd2TNElz

## 2023-03-03

Preference Transformer: Modeling Human Preferences using Transformers for RL

Presents a transformer-based architecture for preference-based RL considering non-Markovian rewards for capturing human preferences over more complex tasks.

https://t.co/kYdZrZMipu https://t.co/A4FTWZiRMD

## 2023-02-27

So what tricks do we have for making stable diffusion better?
- Multi-aspect-ratio training on nice images
- Offset noise
- ControlNet for extra control
- Attend and excite
- Guidance (CLIP, aesthetic, SAG, ?)
- Upscaler
What am I missing? Who has all the above at the moment?

## 2023-02-27

The general disappointment around deep RL is a good illustration of the limitations of deep learning as a medium. RL is a fine idea as a learning paradigm, but it's fundamentally incompatible with curve-fitting. If you're going to do RL don't use deep learning.

## 2023-02-27

Self-forgiveness need not block self-improvement.  You can resolve to do better without constantly berating yourself about the past. Growing pains are about becoming, not regretting.

## 2023-02-27

PatchTST: rather than operating on every individual time step, instead use patching (like in ViTs) to chunk up the time series.

Then, treat the patches as embeddings.

Seems to be one of the most promising transfer learning for time-series techniques. https://t.co/FL5Y0w96d4

## 2023-02-22

All Python üêç¬†developers should know about this module.

The module `collections` will make you 10x more productive*! üöÄ

What is your favourite `collections` tool?

Let me show you some of its goodies in this thread. üëá

(*Actual numbers may vary. ü§°) https://t.co/zrtM0ZeEQr

## 2023-02-20

Just discovered a super useful library to generate LaTeX expression from Python code.

Latexify lets you add LaTex math formulas with only one decorator.

üîóGithub: https://t.co/RT3s4nlWRA
üõ†Ô∏èColab: https://t.co/CZCQumbddC https://t.co/dxtEljJkle

## 2023-02-15

TDSTF: Transformer-based Diffusion probabilistic model for Sparse Time series Forecasting

Paper:
https://t.co/Crth6mgM4U

Python GitHub:
https://t.co/nqA15Ziwnv

## 2023-02-15

IMO the human intuition toward geometry is fairly strongly approximated by LLMs in a sense of token positioning &amp; order.

## 2023-02-08

As a physicist, what I've experienced is that there are these tiny bits of reality which we can model really well with equations. We celebrate their discoveries as heroic achievements because they are so rare. They're not the norm. Most of life is mystery.

## 2023-02-07

I don't think LLMs only solve "typing" (i.e. autocomplete). The way I see it, they solve a broad category of automation problems, where

1. The task medium is natural language
2. Many examples of the task were featured in the training data
3. You don't need &gt;90% accuracy

## 2023-02-05

A fundamental link between logic, computation &amp; geometry is that every truth table is also a binary tree and also a cube. https://t.co/KeoCohXGUq

## 2023-01-31

i'm not convinced cosine similarity is end-game for embedding search

coordinate space is not that interesting on its own

i wish for progress on frequency domain embeddings. can't give up the hunch that harmonic compatibility might have computational advantages (esp. annealing)

## 2023-01-25

Stephen Wolfram on the benefits of blending ChatGPT with WolframAlpha

https://t.co/f812nnOEsT https://t.co/YRgXCjipOn

## 2023-01-24

You don‚Äôt need cheap entertainment, you need meaningful goals, you need to wake up every morning knowing that your life is getting better.

## 2023-01-02

üå† Here are 4 Python causality libraries to learn in 2023 (a thread) 

(1/6)
 
#causaltwitter #causality #causalinference #Python #MachineLearning

## 2023-01-02

Extremely flexible systems rob users of creative freedom. 

It‚Äôs the mistake of thinking rigid axioms prevent one from inventive expression, when it‚Äôs the precise opposite.

One‚Äôs chaos must move against a backdrop of order to create meaning.

## 2022-12-29

If you've wondered - "Which Deep Learning optimizer should I use? SGD? Adagrad? RMSProp?" - this blogpost by @seb_ruder is the best explanation I've seen.

It's a surprisingly easy read! 

https://t.co/ASebqI7N4J 

Definitely a good #100DaysOfMLCode project. https://t.co/fA7YlGavCW

## 2022-11-30

Distribution shifts are not a nuisance to be engineered away - rather, let's make use of them. Natural intelligent systems probably do too. A paper to be presented at NeurIPS today studies this under the assumption that causal mechanisms generating data tend to be robust.

## 2022-11-22

I wrote a history of Bayesian software and the powerful ideas that made it so popular. It's one of those "Tell us about the olden days" things you get asked when you reach a certain age. But I also try and look at what comes next.

https://t.co/MOSbUW6HS7

## 2022-10-28

this is a meta-thread for the keynote address that I gave at the TopoInViz workshop on visualization and topological data analysis, in the IEEE VIZ meeting in OK City October 2022.
thanks to @JulienTierny for inviting me.
i gave a weird talk.
1 https://t.co/UqUzqCOb9n

## 2022-10-26

How to remove your personal contact details from Google results. Good to do it once in a while: https://t.co/ONYtSlcMKK

## 2022-10-19

Just learned more history from prof. Eero Simoncelli of NYU: the score-matching type diffusion/denoising methods had long been used for natural image denoising (at least by Eero's group). The method can actually be traced back to Empirical Bayes statistics in 50's (Miyasawa 1961)

## 2022-10-15

Anyone realizes the (local) expectation of the score match function is exactly the gradient of coding length of the (local) data distribution? That is the same as the gradient of the rate distortion (via lossy coding) -- indicating directions of compressing or expanding the data.

## 2022-10-10

For easy reference:

#Bitcoin https://t.co/Q6ubH35kVR

## 2022-10-06

1/Hyperbolic space significantly enhances deep networks for RL, with near-universal generalization &amp; efficiency benefits in Procgen &amp; Atari, making even PPO and Rainbow competitive with highly-tuned SotA algorithms. 

Check out our new paper: https://t.co/ssdPbVtZQu https://t.co/DYMwjtOHFX

## 2022-10-04

I don't know why, but I'm just not that into the Nobel prize.
It's a weird thing.
If you get a Nobel prize you get a load of clout. But also, the concept of the Nobel prize is absorbing the clout of the scientists who actually did shit

## 2022-10-04

When I teach High-Dim Data Analysis, I always teach  small noise method (aka noisy gradient,  Langevine dynamics, diffusion process), see Lecture 17 of: https://t.co/iMAz9JOIv4  Students asked when it is useful, I said that is the only optimization method I know works in high-dim

## 2022-10-04

Good comparison of topic modeling in this article with embedded Python code: LSA, pLSA, LDA, NMF, BERTopic, Top2Vec

https://t.co/a0djRnDBL3

#NLP https://t.co/HZSMaZqXgG

## 2022-09-25

Diffusion Transformers for Time-Series

https://t.co/ZRDKVRnVaA

## 2022-09-25

Jointly model time-series classifications and their representations with Similarity-Aware Time-Series Classification (SimTSC) using a Graph Neural Network

Paper:
https://t.co/PU2c7Aj0RK

Python GitHub:
https://t.co/VrHQtW8nDl https://t.co/tg9WpTw7af

## 2022-08-24

Appendix:  If you want to learn more, here‚Äôs a reading list that covers diffusion topics.

Iterative denoising processes for image generation: 
https://t.co/2Tp61o4yfy (DDIM)
https://t.co/9VcK9gwap1 (DDPM)
https://t.co/Mg1PicBtle (Score Matching)

## 2022-08-19

At various stages of your life, you start thinking that this time, you have things figured out, then you are proved wrong once again.

Stay humble, or life will humble you.

## 2022-08-18

For beginners looking to get serious about guns and self-defense, this is a 2-part deep dive with @wayofftheres walking you through the first 10+ steps in your journey.  

https://t.co/AoKBvARNJj https://t.co/CPzapbrmfT

## 2022-08-18

Why have diffusion models displaced GANs so quickly?  Consider the tale of the (very strange) first DALLE model.  In 2021, diffusions were almost unheard of, yet the creators of DALLE had already rejected the GAN approach. Here‚Äôs why. üßµ

## 2022-08-10

enjoying this awesome tutorial on conformal prediction by @ml_angelopoulos &amp; @stats_stephen: https://t.co/TBuVvn27yC. what a nice, straightforward framework to think of UQ in practical terms.

there‚Äôs an associated tutorial paper as well: https://t.co/76PNfSIYlJ.

## 2022-01-26

This, by Ravi Vakil, on depicting homological algebra with puzzle pieces, is insanely cool: https://t.co/UcZ70lvQSc https://t.co/CDZ2No2R5C

## 2021-11-29

The material for today's TDA + higher-order dynamics course #complexnetworks2021 is available here https://t.co/eu2eigT2co. I packed together a bunch of different things, so be sure to check out the original repos I ransacked this material from.

## 2021-10-19

@dvassallo @FourFourths Study Markov chains. 

Google "Ashby intro to cybernetics" 

Start from the beginning

## 2021-10-02

How to master complexity and problem solving in science and engineering?

Build insight and understanding first, so that you do not drown in complexity.

Approximations foster understanding.

A thread on Mastering Complexity:

https://t.co/ovbdDnVqhP https://t.co/WADTDRxEHJ

## 2021-08-27

It's time to stop making t-SNE &amp; UMAP plots. In a new preprint w/ Tara Chari we show that while they display some correlation with the underlying high-dimension data, they don't preserve local or global structure &amp; are misleading. They're also arbitrary.üßµhttps://t.co/XkAOTKlOcs https://t.co/dmFzD5RR6R

## 2021-07-21

This is fascinating: Rich Sutton on the "bitter lesson" of AI research: https://t.co/LW5TOGTIKw https://t.co/z4nsRbH01V

## 2021-04-30

New ü¶Ä #rust  #bitcoin tool, https://t.co/DTHpfedQtc reads bitcoin node blocks*.dat files and returns ordered parsed blocks with previous outputs. Scan bitcoin mainnet in 1 hour (needs ~10GB ram, without previous outputs 2.7GB)

## 2021-03-08

In just under 30 seconds, a story with a great question and God‚Äôs even better response: https://t.co/DpEsjJQ5xQ

## 2021-02-15

About love in 36 seconds. https://t.co/YGJBiZcwyF

## 2021-02-03

In less than a minute, the essential message, then and now. https://t.co/PNJ9OdV2yh

## 2020-12-18

Navigate by the stars https://t.co/7hokjhXlsB

## 2020-12-12

I love sensitive people. By this I do not mean fragile, rather people with a mode of consciousness that is characterised by a certain receptivity, openness. They notice, they allow the world to speak to them, they hear &amp; they are capable of true intimacy &amp; closeness.

## 2020-12-09

Forty seconds on how to find the world‚Äôs most valuable treasure. https://t.co/dpAS9SMKFl

## 2020-12-04

You won't have fun doing something by trying to have fun. You'll have fun by being fully present and giving it your best.

## 2020-11-06

When I was younger I read Mises' Theory of Money &amp; Credit and Menger's Principles many times. Great books! I felt I was now a pro at monetary economics. Nope. There was so much more out there. If your voyage into monetary economics stops at Mises &amp; Menger, you're still a rookie.

## 2020-10-28

Self-esteem. https://t.co/sMmvqIOv1o

## 2020-09-26

Every high performer is fundamentally relaxed, and practices minimal use of effort to get a given result. Don't try harder: eliminate tensions.

## 2020-09-04

The problem is not that physicists don‚Äôt appreciate complexity, and the solution is not to avoid wandering into other fields. 

The problem is forgetting that other fields have hard-won expertise, and the solution is to wander as a curious student, not as a valiant savior.

## 2020-08-12

There is a core of what is called "science" (physics, chemistry, almost all of biology and computer science) that is quite sound. Some of the rest of "science" is sound. Most of the rest is quite pathological.

## 2020-07-08

The final step to entering adulthood is realizing that no adults know what they're doing and we're all just trying to figure things out as we stumble through life.

## 2020-07-02

A man does not yield when the mere universe has turned against him; he yields when his own heart has turned against him. We surrender, not when circumstances are miserable, but when we are miserable.

## 2020-06-21

No matter how awesome you think something or someone is, always remember there's someone who's already had it and is totally sick of it.

## 2020-05-12

One of my big regrets is not studying Physics in college. I wish there was an extensive ‚ÄúPhysics for programmers‚Äù course covering classical Physics by building simulations, numerical approximations, and then moving on to theory and closed-form solutions. Not sure if even possible

## 2020-04-28

Bitcoin vs layer2: Liquid vs Lightning is not a one dimensional comparison.

Security: Bitcoin &gt; Liquid &gt; Lightning &gt; Exchange
Trust/uncensorable/unseizable: Bitcoin &gt; Lightning &gt; Liquid &gt; Exchange
Confidentiality: Liquid &gt; Lightning &gt; Bitcoin
Speed: Lightning &gt; Liquid &gt; Bitcoin

## 2020-04-28

Repeat after me:
Indirection is not abstraction.

## 2020-04-08

"Well kept secret: for selfowned people, unlike the rest, the ability to make money (hence make decisions) does not really decline with age." - Nassim Taleb #SecretWisdom

## 2020-02-29

When facing an overwhelming task never think about finishing. Think about starting.

Allow yourself to not finish but force yourself to start.

Once you start you can‚Äôt stop. 
.
.
.
I don‚Äôt know why this works but do I really need to ü§∑üèª‚Äç‚ôÇÔ∏è

## 2020-02-12

I feel like "technical debt" is the wrong metaphor, because with debt you can control the payment schedule.  "Technical land mines" would be more accurate, because the poor technical choices blow up in your face with little advance warning

## 2020-01-20

Most companies:
- over-engineer projects;
- hire ‚Äúrockstars‚Äù;
- name-drop clients; 
- use overly-pretty slide decks;
- talk sales more than products;
- run on over-consensus;
- do more meeting than building.

Start your comp. There‚Äôs nowhere near as much competition as you think.

## 2019-12-29

Asking a physicist to sit through a statistics lecture is like inviting an Italian over for dinner and serving frozen pizza.

## 2019-12-25

Knowledge gained through curiosity is way different than knowledge gained through obligation.

Do not stand in the way of a child's curiosity.

It is her greatest asset.

## 2019-12-14

Procrastination is not bad. 

The idea that you should avoid procrastination is just another example of non-doers pathologizing natural behavior. The kind of BS that comes out of ‚Äúthink tanks.‚Äù

If your work lacks originality or authenticity, try procrastinating.

## 2019-12-14

When you tell me you are a "senior" engineer, I expect you:
Read RFCs
Write documents
Present to your peers
Know how to discuss in writing
Understand tradeoffs
Back arguments with data
Know how to manage meetings
Self manage
Prove there's a problem before you implement a solution

## 2019-12-09

Basically everybody should have a personal website with three things:

1) Three published articles about topics they know well.

2) A list of interests and achievements. 

3) An email sign-up button to capture emails addresses.

You can build your website for free in one weekend.

## 2019-12-05

Intuition is not some special piece of knowledge, it is irreducible perception 

Difficult to articulate because it is not about the parts, but how they all fit together

## 2019-11-19

A Facebook employee who blocks FB for their kid.

An Ivy League professor who has their kid homeschooled / not attend university.

A VC who tells their friends to bootstrap and not take VC funding.

What's the best term for this type of phenomenon?

## 2019-11-13

Agent-based modelling and analytical approaches are not mutually exclusive, especially if the model is not too complicated.

Our RGBM study began with an agent-based model with 100,000,000 agents growing, pooling, sharing resources. Then we solved the model analytically.

## 2019-11-05

The more I improve in any craft, the more humbled I am by just how amazing some people are at it. Competing to be #1 seems ephemeral at best and futile at worst.

In the end, one must find contentment simply in becoming better than one's former self.

## 2019-10-30

WHY YOU SHOULD WRITE

1) Writing moves the world. The Bible, the Constitution, the Gettysburg Address. Study history. The written word is uniquely powerful.

2) Writing sharpens your thinking. The mind usually jumps from thought to thought, but writing forces you to slow down.

## 2019-10-29

In most cases your priority with #bitcoin security should be:

1. Recovery. Don‚Äôt lose your coins.
2. Theft prevention.
3. Privacy.

Don‚Äôt let people intimidate you out of doing #1 well because they overwhelm you with #3.

## 2019-10-29

Differences draw attention.
It takes focus to see similarities.

## 2019-10-28

If your to-read list is in constant expansion, you are spending too much time browsing and too little time reading.

If your to-achieve list is in constant expansion, you are spending too much time observing and too little time doing.

(note to self)

## 2019-10-21

Math is a concise way to represent how a system might behave. It is an anchor with which to fasten rational thought. But simplification is never free. Math is not a lossless compression of nature‚Äôs information.

## 2019-10-17

You become wise not by knowing, but by doing with what you know.

## 2019-10-15

It is not that I am not interested in other applications of crypto, but the separation of money and state seems to be the one with the most far reaching political, social, and economic consequences so I choose to focus on that. Anything that distracts from that just distracts.

## 2019-02-27

Parenting is more about fixing yourself than raising your kids

## 2018-12-26

The three imitators: attachment masquerades as love, pity as compassion and indifference as equanimity.  Our task is to distinguish and to be authentic in loving, feel genuine compassion and exhibit true balance.

## 2018-08-31

Manners are not artificial and rudeness is not authentic. Authenticity is the faithful outward expression of our desires as well as our impulses, and among our desires ought to be consideration for others. #shabbatshalom

## 2018-08-23

Charm is a tactic but kindness is a quality.

